<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Face Attendance System</title>

    <!-- Load PeerJS for connection -->
    <script src="https://unpkg.com/peerjs@1.4.7/dist/peerjs.min.js"></script>
    <!-- Load TensorFlow and Blazeface for Face Detection -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>

    <style>
        :root { --bg: #121212; --surface: #1e1e1e; --primary: #bb86fc; --text: #ffffff; --danger: #cf6679; --success: #03dac6; }
        body { font-family: 'Segoe UI', sans-serif; background: var(--bg); color: var(--text); margin: 0; padding: 0; display: flex; flex-direction: column; height: 100vh; }
        
        header { background: var(--surface); padding: 15px; text-align: center; box-shadow: 0 2px 5px rgba(0,0,0,0.5); }
        h1 { margin: 0; font-size: 1.2rem; }
        
        .container { flex: 1; display: flex; flex-direction: column; padding: 10px; gap: 10px; overflow-y: auto; }
        
        .card { background: var(--surface); padding: 15px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.3); }
        
        button { background: var(--primary); color: #000; border: none; padding: 12px 20px; border-radius: 5px; font-weight: bold; cursor: pointer; width: 100%; margin-top: 10px; font-size: 1rem; }
        button:disabled { background: #555; cursor: not-allowed; }
        button.secondary { background: #333; color: var(--text); border: 1px solid #555; }
        
        input { width: 100%; padding: 10px; margin-top: 5px; background: #333; border: 1px solid #555; color: white; border-radius: 4px; box-sizing: border-box; }
        
        #video-wrapper { position: relative; width: 100%; max-width: 640px; margin: 0 auto; display: none; background: black; border-radius: 8px; overflow: hidden; }
        video { width: 100%; height: auto; display: block; transform: scaleX(-1); } /* Mirror effect */
        canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; transform: scaleX(-1); }
        
        .status-badge { display: inline-block; padding: 5px 10px; border-radius: 4px; font-size: 0.8rem; font-weight: bold; }
        .late { background: var(--danger); color: black; }
        .ontime { background: var(--success); color: black; }
        
        table { width: 100%; border-collapse: collapse; margin-top: 10px; }
        th, td { text-align: left; padding: 10px; border-bottom: 1px solid #333; font-size: 0.9rem; }
        th { color: var(--primary); }
        
        /* Hidden logic pages */
        .page { display: none; }
        .page.active { display: flex; flex-direction: column; gap: 15px; }

        #id-display { font-family: monospace; font-size: 1.2rem; color: var(--primary); letter-spacing: 1px; word-break: break-all;}
    </style>
</head>
<body>

<header>
    <h1>üëÅÔ∏è AI Attendance Monitor</h1>
</header>

<div class="container">

    <!-- SELECT ROLE PAGE -->
    <div id="role-select" class="page active">
        <div class="card">
            <h3>Choose Device Role</h3>
            <p>Are you setting up the Camera (Old Phone) or the Monitor (New Phone)?</p>
            <button onclick="setupCameraMode()">üì∑ Start as Camera (Old Phone)</button>
            <button class="secondary" onclick="setupMonitorMode()">üíª Start as Monitor (New Phone)</button>
        </div>
        <div class="card">
            <h4>Settings</h4>
            <label>Shift Start Time:</label>
            <input type="time" id="shift-start" value="09:00">
        </div>
    </div>

    <!-- CAMERA MODE PAGE -->
    <div id="camera-mode" class="page">
        <div class="card">
            <h3>Camera Active</h3>
            <p>Your ID: <span id="id-display">Generating...</span></p>
            <p style="font-size: 0.8rem; color: #888;">Enter this ID on the monitor device.</p>
        </div>
        <div id="video-wrapper" style="display:block;">
            <video id="local-video" autoplay playsinline muted></video>
            <canvas id="local-canvas"></canvas>
        </div>
        <div class="card">
            <p>Status: <span id="cam-status">Loading AI Model...</span></p>
        </div>
    </div>

    <!-- MONITOR MODE PAGE -->
    <div id="monitor-mode" class="page">
        <div class="card">
            <h3>Connect to Camera</h3>
            <input type="text" id="remote-id-input" placeholder="Paste Camera ID here">
            <button onclick="connectToCamera()">üîó Connect</button>
        </div>
        
        <div id="monitor-view" style="display:none;">
            <div id="video-wrapper" style="display:block;">
                <video id="remote-video" autoplay playsinline></video>
            </div>
            
            <div class="card">
                <h3>Attendance Log</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Employee</th>
                            <th>Clock In</th>
                            <th>Status</th>
                            <th>Duration</th>
                        </tr>
                    </thead>
                    <tbody id="attendance-list">
                        <!-- Rows added by JS -->
                    </tbody>
                </table>
            </div>
        </div>
    </div>

</div>

<script>
    // --- CONFIGURATION ---
    const THRESHOLD_SECONDS = 5; // How long face must be seen to log in
    let shiftStartTime = "09:00"; 
    
    // --- GLOBAL VARS ---
    let peer = null;
    let conn = null;
    let currentStream = null;
    let model = null;
    let isCameraMode = false;
    let detectionInterval = null;
    
    // Attendance State
    let activeSession = null; // { startTime: Date, name: "Employee 1" }
    let attendanceLog = [];

    // --- DOM ELEMENTS ---
    const pages = document.querySelectorAll('.page');
    const localVideo = document.getElementById('local-video');
    const localCanvas = document.getElementById('local-canvas');
    const remoteVideo = document.getElementById('remote-video');
    const statusText = document.getElementById('cam-status');
    const listBody = document.getElementById('attendance-list');

    // --- NAVIGATION ---
    function showPage(id) {
        pages.forEach(p => p.classList.remove('active'));
        document.getElementById(id).classList.add('active');
    }

    document.getElementById('shift-start').addEventListener('change', (e) => {
        shiftStartTime = e.target.value;
    });

    // ==========================================
    // üì∑ CAMERA MODE (OLD PHONE)
    // ==========================================
    async function setupCameraMode() {
        showPage('camera-mode');
        isCameraMode = true;

        // 1. Initialize PeerJS
        peer = new Peer(); 
        
        peer.on('open', (id) => {
            document.getElementById('id-display').innerText = id;
            statusText.innerText = "Waiting for connection...";
        });

        peer.on('connection', (c) => {
            conn = c;
            statusText.innerText = "Monitor Connected!";
            handleIncomingData();
        });

        // 2. Start Camera
        try {
            currentStream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio: false });
            localVideo.srcObject = currentStream;
            
            // Wait for video to load metadata
            localVideo.onloadedmetadata = () => {
                localCanvas.width = localVideo.videoWidth;
                localCanvas.height = localVideo.videoHeight;
                loadFaceModel(); // Start AI
            };
        } catch (err) {
            alert("Camera access denied or not available. Use HTTPS.");
            console.error(err);
        }

        // 3. Handle Call (Video Stream)
        peer.on('call', (call) => {
            call.answer(currentStream); // Answer with our stream
        });
    }

    async function loadFaceModel() {
        statusText.innerText = "Loading AI Model...";
        model = await blazeface.load();
        statusText.innerText = "AI Active. Scanning...";
        detectFaces();
    }

    async function detectFaces() {
        if (!isCameraMode) return;

        const ctx = localCanvas.getContext('2d');
        
        // Loop
        setInterval(async () => {
            if (model && localVideo.readyState === 4) {
                // Detect faces
                const predictions = await model.estimateFaces(localVideo, false);

                // Clear canvas
                ctx.clearRect(0, 0, localCanvas.width, localCanvas.height);

                if (predictions.length > 0) {
                    // Draw box
                    predictions.forEach(prediction => {
                        const start = prediction.topLeft;
                        const end = prediction.bottomRight;
                        const size = [end[0] - start[0], end[1] - start[1]];
                        
                        ctx.beginPath();
                        ctx.lineWidth = "4";
                        ctx.strokeStyle = "#03dac6";
                        ctx.rect(start[0], start[1], size[0], size[1]);
                        ctx.stroke();
                    });

                    // Send signal to monitor "Face Detected"
                    if (conn && conn.open) {
                        conn.send({ type: 'FACE_DETECTED', timestamp: new Date().getTime() });
                    }
                }
            }
        }, 500); // Check twice a second
    }

    // ==========================================
    // üíª MONITOR MODE (NEW PHONE)
    // ==========================================
    function setupMonitorMode() {
        showPage('monitor-mode');
        peer = new Peer();
        peer.on('open', (id) => {
            console.log('Monitor Peer ID: ' + id);
        });
    }

    function connectToCamera() {
        const remoteId = document.getElementById('remote-id-input').value;
        if (!remoteId) return alert("Enter Camera ID");

        // 1. Connect Data Channel
        conn = peer.connect(remoteId);
        
        conn.on('open', () => {
            alert("Connected to Camera System!");
            document.querySelector('.card h3').innerText = "Connected";
            document.getElementById('monitor-view').style.display = 'block';
            
            // 2. Call to get Video
            const call = peer.call(remoteId, new MediaStream()); // Send empty stream to get one back
            call.on('stream', (remoteStream) => {
                remoteVideo.srcObject = remoteStream;
            });
        });

        conn.on('data', (data) => {
            if (data.type === 'FACE_DETECTED') {
                handleFaceDetection(data.timestamp);
            }
        });
    }

    // ==========================================
    // üß† LOGIC & ATTENDANCE
    // ==========================================
    
    // Logic: If we see a face, assume it's "Employee #1" (since we don't have a DB for recognition)
    // If they are already clocked in, update timer. If not, clock them in.
    
    let lastDetectionTime = 0;
    
    function handleFaceDetection(timestamp) {
        const now = new Date().getTime();
        lastDetectionTime = now;

        // If no active session, start one
        if (!activeSession) {
            clockIn();
        }
    }

    function clockIn() {
        const now = new Date();
        const hh = String(now.getHours()).padStart(2, '0');
        const mm = String(now.getMinutes()).padStart(2, '0');
        const currentTimeString = `${hh}:${mm}`;

        // Determine Late Status
        let status = "On Time";
        let statusClass = "ontime";
        
        if (currentTimeString > shiftStartTime) {
            status = "LATE";
            statusClass = "late";
        }

        // Create Session Object
        activeSession = {
            id: Date.now(),
            name: "Employee " + (attendanceLog.length + 1), // Simple auto-naming
            startTime: now,
            status: status,
            statusClass: statusClass
        };

        attendanceLog.push(activeSession);
        renderTable();
        
        // Start Timer updater
        if (!detectionInterval) {
            detectionInterval = setInterval(updateTimers, 1000);
        }
    }

    function updateTimers() {
        // If we haven't seen a face for 30 seconds, consider them "Away" (optional logic, keeping it simple here)
        // Here we just update the "Duration" text on the UI
        renderTable();
    }

    function renderTable() {
        listBody.innerHTML = "";
        
        // In reverse order (newest top)
        [...attendanceLog].reverse().forEach(session => {
            const now = new Date();
            const diffMs = now - session.startTime;
            
            // Format Duration
            const diffHrs = Math.floor((diffMs % 86400000) / 3600000);
            const diffMins = Math.floor(((diffMs % 86400000) % 3600000) / 60000);
            const diffSecs = Math.floor((((diffMs % 86400000) % 3600000) % 60000) / 1000);
            const durationStr = `${diffHrs}h ${diffMins}m ${diffSecs}s`;

            const row = `
                <tr>
                    <td><input type="text" value="${session.name}" style="border:none; background:transparent; color:white; font-weight:bold;" onchange="updateName(${session.id}, this.value)"></td>
                    <td>${session.startTime.toLocaleTimeString()}</td>
                    <td><span class="status-badge ${session.statusClass}">${session.status}</span></td>
                    <td>${durationStr}</td>
                </tr>
            `;
            listBody.innerHTML += row;
        });
    }

    function updateName(id, newName) {
        const s = attendanceLog.find(x => x.id === id);
        if(s) s.name = newName;
    }

    // Helper to send data (Placeholder if needed in reverse)
    function handleIncomingData() {
        // Camera doesn't need to handle much incoming data in this version
    }

</script>
</body>
</html>
